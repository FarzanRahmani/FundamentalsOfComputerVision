{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data From Drive"
      ],
      "metadata": {
        "id": "sudP5h2cEV8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"19FAA4_z7xzdqqtwnkLRL2oI4PuMWPYBZ&confirm=t\"\n",
        "!gdown \"1ixS6ump2gFwwChlg4vrGyVa_9t0csLKd&confirm=t\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSiJ-7FoB4ii",
        "outputId": "e4500df3-3e83-4057-ee2e-03942782c931"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19FAA4_z7xzdqqtwnkLRL2oI4PuMWPYBZ&confirm=t\n",
            "To: /content/Dataset.zip\n",
            "100% 845M/845M [00:09<00:00, 88.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ixS6ump2gFwwChlg4vrGyVa_9t0csLKd&confirm=t\n",
            "To: /content/Patterns.zip\n",
            "100% 271M/271M [00:02<00:00, 122MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Dataset.zip -d Dataset\n",
        "!unzip /content/Patterns.zip -d Patterns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36W_AWgWHa6L",
        "outputId": "b9bf967b-94c7-45aa-801a-8da0e2c771d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Dataset.zip\n",
            " extracting: Dataset/1644359422.816138.jpg  \n",
            "  inflating: Dataset/1644359422.816138.json  \n",
            " extracting: Dataset/1644360063.82364.jpg  \n",
            "  inflating: Dataset/1644360063.82364.json  \n",
            " extracting: Dataset/1644362482.7094185.jpg  \n",
            "  inflating: Dataset/1644362482.7094185.json  \n",
            " extracting: Dataset/1644368971.6154966.jpg  \n",
            "  inflating: Dataset/1644368971.6154966.json  \n",
            " extracting: Dataset/1644369207.922902.jpg  \n",
            "  inflating: Dataset/1644369207.922902.json  \n",
            " extracting: Dataset/1644369658.7214494.jpg  \n",
            "  inflating: Dataset/1644369658.7214494.json  \n",
            " extracting: Dataset/1644369774.9055796.jpg  \n",
            "  inflating: Dataset/1644369774.9055796.json  \n",
            " extracting: Dataset/1644369801.9909956.jpg  \n",
            "  inflating: Dataset/1644369801.9909956.json  \n",
            " extracting: Dataset/1644369842.0238864.jpg  \n",
            "  inflating: Dataset/1644369842.0238864.json  \n",
            " extracting: Dataset/1644369905.2248352.jpg  \n",
            "  inflating: Dataset/1644369905.2248352.json  \n",
            " extracting: Dataset/1644369972.1056488.jpg  \n",
            "  inflating: Dataset/1644369972.1056488.json  \n",
            " extracting: Dataset/1644370118.3895395.jpg  \n",
            "  inflating: Dataset/1644370118.3895395.json  \n",
            " extracting: Dataset/1644370205.1133745.jpg  \n",
            "  inflating: Dataset/1644370205.1133745.json  \n",
            " extracting: Dataset/1644370633.900776.jpg  \n",
            "  inflating: Dataset/1644370633.900776.json  \n",
            " extracting: Dataset/1644370783.654187.jpg  \n",
            "  inflating: Dataset/1644370783.654187.json  \n",
            " extracting: Dataset/1644370982.1721344.jpg  \n",
            "  inflating: Dataset/1644370982.1721344.json  \n",
            " extracting: Dataset/1644370989.7056713.jpg  \n",
            "  inflating: Dataset/1644370989.7056713.json  \n",
            " extracting: Dataset/1644371201.4568157.jpg  \n",
            "  inflating: Dataset/1644371201.4568157.json  \n",
            " extracting: Dataset/1644371210.8952444.jpg  \n",
            "  inflating: Dataset/1644371210.8952444.json  \n",
            " extracting: Dataset/1644371253.0164495.jpg  \n",
            "  inflating: Dataset/1644371253.0164495.json  \n",
            " extracting: Dataset/1644371311.8005233.jpg  \n",
            "  inflating: Dataset/1644371311.8005233.json  \n",
            " extracting: Dataset/1644371541.792311.jpg  \n",
            "  inflating: Dataset/1644371541.792311.json  \n",
            " extracting: Dataset/1644371590.2467813.jpg  \n",
            "  inflating: Dataset/1644371590.2467813.json  \n",
            " extracting: Dataset/1644371605.94304.jpg  \n",
            "  inflating: Dataset/1644371605.94304.json  \n",
            " extracting: Dataset/1644371756.12533.jpg  \n",
            "  inflating: Dataset/1644371756.12533.json  \n",
            " extracting: Dataset/1644371889.2719016.jpg  \n",
            "  inflating: Dataset/1644371889.2719016.json  \n",
            " extracting: Dataset/1644371937.6880414.jpg  \n",
            "  inflating: Dataset/1644371937.6880414.json  \n",
            " extracting: Dataset/1644371982.7835793.jpg  \n",
            "  inflating: Dataset/1644371982.7835793.json  \n",
            " extracting: Dataset/1644371999.918624.jpg  \n",
            "  inflating: Dataset/1644371999.918624.json  \n",
            " extracting: Dataset/1644372134.2164211.jpg  \n",
            "  inflating: Dataset/1644372134.2164211.json  \n",
            " extracting: Dataset/1644372183.0533073.jpg  \n",
            "  inflating: Dataset/1644372183.0533073.json  \n",
            " extracting: Dataset/1644372193.6399484.jpg  \n",
            "  inflating: Dataset/1644372193.6399484.json  \n",
            " extracting: Dataset/1644372810.8630261.jpg  \n",
            "  inflating: Dataset/1644372810.8630261.json  \n",
            " extracting: Dataset/1644372936.2954907.jpg  \n",
            "  inflating: Dataset/1644372936.2954907.json  \n",
            " extracting: Dataset/1644372956.9503222.jpg  \n",
            "  inflating: Dataset/1644372956.9503222.json  \n",
            " extracting: Dataset/1644374941.8222482.jpg  \n",
            "  inflating: Dataset/1644374941.8222482.json  \n",
            " extracting: Dataset/1644376439.1605.jpg  \n",
            "  inflating: Dataset/1644376439.1605.json  \n",
            " extracting: Dataset/1644377103.9925284.jpg  \n",
            "  inflating: Dataset/1644377103.9925284.json  \n",
            " extracting: Dataset/1644377489.3615768.jpg  \n",
            "  inflating: Dataset/1644377489.3615768.json  \n",
            " extracting: Dataset/1644378336.3014474.jpg  \n",
            "  inflating: Dataset/1644378336.3014474.json  \n",
            " extracting: Dataset/1644379102.917364.jpg  \n",
            "  inflating: Dataset/1644379102.917364.json  \n",
            " extracting: Dataset/1644379252.7150154.jpg  \n",
            "  inflating: Dataset/1644379252.7150154.json  \n",
            " extracting: Dataset/1644380448.16593.jpg  \n",
            "  inflating: Dataset/1644380448.16593.json  \n",
            " extracting: Dataset/1644380451.0256197.jpg  \n",
            "  inflating: Dataset/1644380451.0256197.json  \n",
            " extracting: Dataset/1644381305.8431911.jpg  \n",
            "  inflating: Dataset/1644381305.8431911.json  \n",
            " extracting: Dataset/1644381808.1046782.jpg  \n",
            "  inflating: Dataset/1644381808.1046782.json  \n",
            " extracting: Dataset/1644381826.675793.jpg  \n",
            "  inflating: Dataset/1644381826.675793.json  \n",
            " extracting: Dataset/1644382033.2120235.jpg  \n",
            "  inflating: Dataset/1644382033.2120235.json  \n",
            " extracting: Dataset/1644382665.3380032.jpg  \n",
            "  inflating: Dataset/1644382665.3380032.json  \n",
            " extracting: Dataset/1644385138.469252.jpg  \n",
            "  inflating: Dataset/1644385138.469252.json  \n",
            " extracting: Dataset/1644385189.3186276.jpg  \n",
            "  inflating: Dataset/1644385189.3186276.json  \n",
            " extracting: Dataset/1644385398.441569.jpg  \n",
            "  inflating: Dataset/1644385398.441569.json  \n",
            " extracting: Dataset/1644385445.7715538.jpg  \n",
            "  inflating: Dataset/1644385445.7715538.json  \n",
            " extracting: Dataset/1644385793.3430195.jpg  \n",
            "  inflating: Dataset/1644385793.3430195.json  \n",
            " extracting: Dataset/1644385988.2358375.jpg  \n",
            "  inflating: Dataset/1644385988.2358375.json  \n",
            " extracting: Dataset/1644386314.3869328.jpg  \n",
            "  inflating: Dataset/1644386314.3869328.json  \n",
            " extracting: Dataset/1644386673.006881.jpg  \n",
            "  inflating: Dataset/1644386673.006881.json  \n",
            " extracting: Dataset/1644386772.4910994.jpg  \n",
            "  inflating: Dataset/1644386772.4910994.json  \n",
            " extracting: Dataset/1644386798.7467856.jpg  \n",
            "  inflating: Dataset/1644386798.7467856.json  \n",
            " extracting: Dataset/1644387395.0814912.jpg  \n",
            "  inflating: Dataset/1644387395.0814912.json  \n",
            " extracting: Dataset/1644387425.1517851.jpg  \n",
            "  inflating: Dataset/1644387425.1517851.json  \n",
            " extracting: Dataset/1644387674.0154285.jpg  \n",
            "  inflating: Dataset/1644387674.0154285.json  \n",
            " extracting: Dataset/1644387973.939511.jpg  \n",
            "  inflating: Dataset/1644387973.939511.json  \n",
            " extracting: Dataset/1644388718.0461798.jpg  \n",
            "  inflating: Dataset/1644388718.0461798.json  \n",
            " extracting: Dataset/1644389142.8664496.jpg  \n",
            "  inflating: Dataset/1644389142.8664496.json  \n",
            " extracting: Dataset/1644389271.051818.jpg  \n",
            "  inflating: Dataset/1644389271.051818.json  \n",
            " extracting: Dataset/1644393778.6251702.jpg  \n",
            "  inflating: Dataset/1644393778.6251702.json  \n",
            " extracting: Dataset/1644393916.8820436.jpg  \n",
            "  inflating: Dataset/1644393916.8820436.json  \n",
            " extracting: Dataset/1644393954.5714774.jpg  \n",
            "  inflating: Dataset/1644393954.5714774.json  \n",
            " extracting: Dataset/1644398023.8130991.jpg  \n",
            "  inflating: Dataset/1644398023.8130991.json  \n",
            " extracting: Dataset/1644398492.8470697.jpg  \n",
            "  inflating: Dataset/1644398492.8470697.json  \n",
            " extracting: Dataset/1644398600.9163294.jpg  \n",
            "  inflating: Dataset/1644398600.9163294.json  \n",
            " extracting: Dataset/1644398687.462042.jpg  \n",
            "  inflating: Dataset/1644398687.462042.json  \n",
            " extracting: Dataset/1644398700.8275552.jpg  \n",
            "  inflating: Dataset/1644398700.8275552.json  \n",
            " extracting: Dataset/1644398993.4711878.jpg  \n",
            "  inflating: Dataset/1644398993.4711878.json  \n",
            " extracting: Dataset/1644399283.449887.jpg  \n",
            "  inflating: Dataset/1644399283.449887.json  \n",
            " extracting: Dataset/1644399350.3674173.jpg  \n",
            "  inflating: Dataset/1644399350.3674173.json  \n",
            " extracting: Dataset/1644399881.0177782.jpg  \n",
            "  inflating: Dataset/1644399881.0177782.json  \n",
            " extracting: Dataset/1644401509.7809024.jpg  \n",
            "  inflating: Dataset/1644401509.7809024.json  \n",
            " extracting: Dataset/1644401866.3145134.jpg  \n",
            "  inflating: Dataset/1644401866.3145134.json  \n",
            " extracting: Dataset/1644401875.126233.jpg  \n",
            "  inflating: Dataset/1644401875.126233.json  \n",
            " extracting: Dataset/1644401882.101451.jpg  \n",
            "  inflating: Dataset/1644401882.101451.json  \n",
            " extracting: Dataset/1644402032.988678.jpg  \n",
            "  inflating: Dataset/1644402032.988678.json  \n",
            " extracting: Dataset/1644402109.934155.jpg  \n",
            "  inflating: Dataset/1644402109.934155.json  \n",
            " extracting: Dataset/1644404239.1987379.jpg  \n",
            "  inflating: Dataset/1644404239.1987379.json  \n",
            " extracting: Dataset/1644404487.231225.jpg  \n",
            "  inflating: Dataset/1644404487.231225.json  \n",
            " extracting: Dataset/1644404598.9965065.jpg  \n",
            "  inflating: Dataset/1644404598.9965065.json  \n",
            " extracting: Dataset/1644405365.7739215.jpg  \n",
            "  inflating: Dataset/1644405365.7739215.json  \n",
            " extracting: Dataset/1644405827.5746589.jpg  \n",
            "  inflating: Dataset/1644405827.5746589.json  \n",
            "  inflating: Dataset/1644422683.4851024.json  \n",
            "  inflating: Dataset/1644422683.4851024.png  \n",
            "  inflating: Dataset/1644422745.642237.json  \n",
            "  inflating: Dataset/1644422745.642237.png  \n",
            "  inflating: Dataset/1644422806.012347.json  \n",
            "  inflating: Dataset/1644422806.012347.png  \n",
            "  inflating: Dataset/1644422831.9930315.json  \n",
            "  inflating: Dataset/1644422831.9930315.png  \n",
            "  inflating: Dataset/1644422856.258654.json  \n",
            "  inflating: Dataset/1644422856.258654.png  \n",
            "  inflating: Dataset/1644422878.9324076.json  \n",
            "  inflating: Dataset/1644422878.9324076.png  \n",
            "  inflating: Dataset/1644422921.1765754.json  \n",
            "  inflating: Dataset/1644422921.1765754.png  \n",
            "  inflating: Dataset/1644422935.2311373.json  \n",
            "  inflating: Dataset/1644422935.2311373.png  \n",
            "  inflating: Dataset/1644422961.626815.json  \n",
            "  inflating: Dataset/1644422961.626815.png  \n",
            "  inflating: Dataset/1644423354.439917.json  \n",
            "  inflating: Dataset/1644423354.439917.png  \n",
            "  inflating: Dataset/1644423376.295767.json  \n",
            "  inflating: Dataset/1644423376.295767.png  \n",
            "  inflating: Dataset/1644423543.3183112.json  \n",
            "  inflating: Dataset/1644423543.3183112.png  \n",
            "  inflating: Dataset/1644425053.5815506.json  \n",
            "  inflating: Dataset/1644425053.5815506.png  \n",
            "  inflating: Dataset/1644425061.6781607.json  \n",
            "  inflating: Dataset/1644425061.6781607.png  \n",
            "  inflating: Dataset/1644425196.4281936.json  \n",
            "  inflating: Dataset/1644425196.4281936.png  \n",
            "  inflating: Dataset/1644425199.5427296.json  \n",
            "  inflating: Dataset/1644425199.5427296.png  \n",
            "  inflating: Dataset/1644425376.9260497.json  \n",
            "  inflating: Dataset/1644425376.9260497.png  \n",
            "  inflating: Dataset/1644425466.1547256.json  \n",
            "  inflating: Dataset/1644425466.1547256.png  \n",
            "  inflating: Dataset/1644426304.222298.json  \n",
            "  inflating: Dataset/1644426304.222298.png  \n",
            "  inflating: Dataset/1644426362.5091567.json  \n",
            "  inflating: Dataset/1644426362.5091567.png  \n",
            "  inflating: Dataset/1644426830.621672.json  \n",
            "  inflating: Dataset/1644426830.621672.png  \n",
            "  inflating: Dataset/1644426841.009254.json  \n",
            "  inflating: Dataset/1644426841.009254.png  \n",
            "  inflating: Dataset/1644426961.8556552.json  \n",
            "  inflating: Dataset/1644426961.8556552.png  \n",
            "  inflating: Dataset/1644427019.1058862.json  \n",
            "  inflating: Dataset/1644427019.1058862.png  \n",
            "  inflating: Dataset/1644427027.5875607.json  \n",
            "  inflating: Dataset/1644427027.5875607.png  \n",
            "  inflating: Dataset/1644427076.3319407.json  \n",
            "  inflating: Dataset/1644427076.3319407.png  \n",
            "  inflating: Dataset/1644427085.700158.json  \n",
            "  inflating: Dataset/1644427085.700158.png  \n",
            "  inflating: Dataset/1644427134.149282.json  \n",
            "  inflating: Dataset/1644427134.149282.png  \n",
            "  inflating: Dataset/1644427145.245704.json  \n",
            "  inflating: Dataset/1644427145.245704.png  \n",
            "  inflating: Dataset/1644427148.2046278.json  \n",
            "  inflating: Dataset/1644427148.2046278.png  \n",
            "  inflating: Dataset/1644427485.8393977.json  \n",
            "  inflating: Dataset/1644427485.8393977.png  \n",
            "  inflating: Dataset/1644427493.3979545.json  \n",
            "  inflating: Dataset/1644427493.3979545.png  \n",
            "  inflating: Dataset/1644427531.4051683.json  \n",
            "  inflating: Dataset/1644427531.4051683.png  \n",
            "  inflating: Dataset/1644427556.140234.json  \n",
            "  inflating: Dataset/1644427556.140234.png  \n",
            "  inflating: Dataset/1644427558.9166248.json  \n",
            "  inflating: Dataset/1644427558.9166248.png  \n",
            "  inflating: Dataset/1644427592.7607744.json  \n",
            "  inflating: Dataset/1644427592.7607744.png  \n",
            "  inflating: Dataset/1644427637.7261326.json  \n",
            "  inflating: Dataset/1644427637.7261326.png  \n",
            "  inflating: Dataset/1644427659.245246.json  \n",
            "  inflating: Dataset/1644427659.245246.png  \n",
            "  inflating: Dataset/1644427828.5770006.json  \n",
            "  inflating: Dataset/1644427828.5770006.png  \n",
            "  inflating: Dataset/1644427902.328577.json  \n",
            "  inflating: Dataset/1644427902.328577.png  \n",
            "  inflating: Dataset/1644427904.9565911.json  \n",
            "  inflating: Dataset/1644427904.9565911.png  \n",
            "  inflating: Dataset/1644427937.9748037.json  \n",
            "  inflating: Dataset/1644427937.9748037.png  \n",
            "  inflating: Dataset/1644428080.0388353.json  \n",
            "  inflating: Dataset/1644428080.0388353.png  \n",
            "  inflating: Dataset/1644428102.8042848.json  \n",
            "  inflating: Dataset/1644428102.8042848.png  \n",
            "  inflating: Dataset/1644428145.109586.json  \n",
            "  inflating: Dataset/1644428145.109586.png  \n",
            "  inflating: Dataset/1644428230.99102.json  \n",
            "  inflating: Dataset/1644428230.99102.png  \n",
            "  inflating: Dataset/1644428236.9963467.json  \n",
            "  inflating: Dataset/1644428236.9963467.png  \n",
            "  inflating: Dataset/1644428265.7917786.json  \n",
            "  inflating: Dataset/1644428265.7917786.png  \n",
            "  inflating: Dataset/1644428351.2051973.json  \n",
            "  inflating: Dataset/1644428351.2051973.png  \n",
            "  inflating: Dataset/1644428374.0907235.json  \n",
            "  inflating: Dataset/1644428374.0907235.png  \n",
            "  inflating: Dataset/1644428808.1839433.json  \n",
            "  inflating: Dataset/1644428808.1839433.png  \n",
            "  inflating: Dataset/1644428916.8798635.json  \n",
            "  inflating: Dataset/1644428916.8798635.png  \n",
            "  inflating: Dataset/1644428957.483542.json  \n",
            "  inflating: Dataset/1644428957.483542.png  \n",
            "  inflating: Dataset/1644429216.0664825.json  \n",
            "  inflating: Dataset/1644429216.0664825.png  \n",
            "  inflating: Dataset/1644429218.4574702.json  \n",
            "  inflating: Dataset/1644429218.4574702.png  \n",
            "  inflating: Dataset/1644429230.3097088.json  \n",
            "  inflating: Dataset/1644429230.3097088.png  \n",
            "  inflating: Dataset/1644429250.3943634.json  \n",
            "  inflating: Dataset/1644429250.3943634.png  \n",
            "  inflating: Dataset/1644429286.557027.json  \n",
            "  inflating: Dataset/1644429286.557027.png  \n",
            "  inflating: Dataset/1644429291.6177268.json  \n",
            "  inflating: Dataset/1644429291.6177268.png  \n",
            "  inflating: Dataset/1644429319.7578897.json  \n",
            "  inflating: Dataset/1644429319.7578897.png  \n",
            "  inflating: Dataset/1644429334.9533093.json  \n",
            "  inflating: Dataset/1644429334.9533093.png  \n",
            "  inflating: Dataset/1644429511.39629.json  \n",
            "  inflating: Dataset/1644429511.39629.png  \n",
            "  inflating: Dataset/1644429541.759793.json  \n",
            "  inflating: Dataset/1644429541.759793.png  \n",
            "  inflating: Dataset/1644429552.2189991.json  \n",
            "  inflating: Dataset/1644429552.2189991.png  \n",
            "  inflating: Dataset/1644429910.9062922.json  \n",
            "  inflating: Dataset/1644429910.9062922.png  \n",
            "  inflating: Dataset/1644429949.997164.json  \n",
            "  inflating: Dataset/1644429949.997164.png  \n",
            "  inflating: Dataset/1644430038.7515485.json  \n",
            "  inflating: Dataset/1644430038.7515485.png  \n",
            "  inflating: Dataset/1644430209.2669516.json  \n",
            "  inflating: Dataset/1644430209.2669516.png  \n",
            "  inflating: Dataset/1644430597.137529.json  \n",
            "  inflating: Dataset/1644430597.137529.png  \n",
            "  inflating: Dataset/1644430667.725543.json  \n",
            "  inflating: Dataset/1644430667.725543.png  \n",
            "  inflating: Dataset/1644432267.496155.json  \n",
            "  inflating: Dataset/1644432267.496155.png  \n",
            "  inflating: Dataset/1644432281.2365892.json  \n",
            "  inflating: Dataset/1644432281.2365892.png  \n",
            "  inflating: Dataset/1644432291.4884074.json  \n",
            "  inflating: Dataset/1644432291.4884074.png  \n",
            "  inflating: Dataset/1644432296.0410187.json  \n",
            "  inflating: Dataset/1644432296.0410187.png  \n",
            "  inflating: Dataset/1644432906.779638.json  \n",
            "  inflating: Dataset/1644432906.779638.png  \n",
            "  inflating: Dataset/1644432952.3871663.json  \n",
            "  inflating: Dataset/1644432952.3871663.png  \n",
            "  inflating: Dataset/1644433280.1804423.json  \n",
            "  inflating: Dataset/1644433280.1804423.png  \n",
            "  inflating: Dataset/1644433459.6740515.json  \n",
            "  inflating: Dataset/1644433459.6740515.png  \n",
            "  inflating: Dataset/1644433481.9765427.json  \n",
            "  inflating: Dataset/1644433481.9765427.png  \n",
            "  inflating: Dataset/1644433552.6248033.json  \n",
            "  inflating: Dataset/1644433552.6248033.png  \n",
            "  inflating: Dataset/1644433790.3127308.json  \n",
            "  inflating: Dataset/1644433790.3127308.png  \n",
            "  inflating: Dataset/1644433821.0196214.json  \n",
            "  inflating: Dataset/1644433821.0196214.png  \n",
            "  inflating: Dataset/1644434202.603224.json  \n",
            "  inflating: Dataset/1644434202.603224.png  \n",
            "  inflating: Dataset/1644434369.4772506.json  \n",
            "  inflating: Dataset/1644434369.4772506.png  \n",
            "  inflating: Dataset/1644434379.7003157.json  \n",
            "  inflating: Dataset/1644434379.7003157.png  \n",
            "  inflating: Dataset/1644434400.6770675.json  \n",
            "  inflating: Dataset/1644434400.6770675.png  \n",
            "  inflating: Dataset/1644434445.597476.json  \n",
            "  inflating: Dataset/1644434445.597476.png  \n",
            "  inflating: Dataset/1644434639.1050997.json  \n",
            "  inflating: Dataset/1644434639.1050997.png  \n",
            "  inflating: Dataset/1644435428.392757.json  \n",
            "  inflating: Dataset/1644435428.392757.png  \n",
            "  inflating: Dataset/1644435639.6224587.json  \n",
            "  inflating: Dataset/1644435639.6224587.png  \n",
            "  inflating: Dataset/1644435666.3256445.json  \n",
            "  inflating: Dataset/1644435666.3256445.png  \n",
            "  inflating: Dataset/1644435683.4366465.json  \n",
            "  inflating: Dataset/1644435683.4366465.png  \n",
            "  inflating: Dataset/1644435693.3563437.json  \n",
            "  inflating: Dataset/1644435693.3563437.png  \n",
            "  inflating: Dataset/1644435833.77119.json  \n",
            "  inflating: Dataset/1644435833.77119.png  \n",
            "  inflating: Dataset/1644436152.2717178.json  \n",
            "  inflating: Dataset/1644436152.2717178.png  \n",
            "  inflating: Dataset/1644436161.5902526.json  \n",
            "  inflating: Dataset/1644436161.5902526.png  \n",
            "  inflating: Dataset/1644436846.7489126.json  \n",
            "  inflating: Dataset/1644436846.7489126.png  \n",
            "  inflating: Dataset/1644437189.2882862.json  \n",
            "  inflating: Dataset/1644437189.2882862.png  \n",
            "  inflating: Dataset/1644437279.5770764.json  \n",
            "  inflating: Dataset/1644437279.5770764.png  \n",
            "  inflating: Dataset/1644437317.2406604.json  \n",
            "  inflating: Dataset/1644437317.2406604.png  \n",
            "  inflating: Dataset/1644437322.3866885.json  \n",
            "  inflating: Dataset/1644437322.3866885.png  \n",
            "  inflating: Dataset/1644437377.1876879.json  \n",
            "  inflating: Dataset/1644437377.1876879.png  \n",
            "  inflating: Dataset/1644437400.8946483.json  \n",
            "  inflating: Dataset/1644437400.8946483.png  \n",
            "  inflating: Dataset/1644437729.2224987.json  \n",
            "  inflating: Dataset/1644437729.2224987.png  \n",
            "  inflating: Dataset/1644437931.7794275.json  \n",
            "  inflating: Dataset/1644437931.7794275.png  \n",
            "  inflating: Dataset/1644438404.7145946.json  \n",
            "  inflating: Dataset/1644438404.7145946.png  \n",
            "  inflating: Dataset/1644439046.1075752.json  \n",
            "  inflating: Dataset/1644439046.1075752.png  \n",
            "  inflating: Dataset/1644439065.1931672.json  \n",
            "  inflating: Dataset/1644439065.1931672.png  \n",
            "  inflating: Dataset/1644439177.8135765.json  \n",
            "  inflating: Dataset/1644439177.8135765.png  \n",
            "  inflating: Dataset/1644439227.3692305.json  \n",
            "  inflating: Dataset/1644439227.3692305.png  \n",
            "  inflating: Dataset/1644439245.3811543.json  \n",
            "  inflating: Dataset/1644439245.3811543.png  \n",
            "  inflating: Dataset/1644439331.6699107.json  \n",
            "  inflating: Dataset/1644439331.6699107.png  \n",
            "  inflating: Dataset/1644439410.650488.json  \n",
            "  inflating: Dataset/1644439410.650488.png  \n",
            "  inflating: Dataset/1644439819.868499.json  \n",
            "  inflating: Dataset/1644439819.868499.png  \n",
            "  inflating: Dataset/1644439867.6478252.json  \n",
            "  inflating: Dataset/1644439867.6478252.png  \n",
            "  inflating: Dataset/1644439872.5570402.json  \n",
            "  inflating: Dataset/1644439872.5570402.png  \n",
            "  inflating: Dataset/1644440462.8398256.json  \n",
            "  inflating: Dataset/1644440462.8398256.png  \n",
            "  inflating: Dataset/1644440480.5062313.json  \n",
            "  inflating: Dataset/1644440480.5062313.png  \n",
            "  inflating: Dataset/1644440590.2795231.json  \n",
            "  inflating: Dataset/1644440590.2795231.png  \n",
            "  inflating: Dataset/1644440640.7741735.json  \n",
            "  inflating: Dataset/1644440640.7741735.png  \n",
            "  inflating: Dataset/1644440693.2555249.json  \n",
            "  inflating: Dataset/1644440693.2555249.png  \n",
            "  inflating: Dataset/1644440713.6423626.json  \n",
            "  inflating: Dataset/1644440713.6423626.png  \n",
            "  inflating: Dataset/1644440976.5123446.json  \n",
            "  inflating: Dataset/1644440976.5123446.png  \n",
            "  inflating: Dataset/1644441121.0958824.json  \n",
            "  inflating: Dataset/1644441121.0958824.png  \n",
            "  inflating: Dataset/1644441231.4249313.json  \n",
            "  inflating: Dataset/1644441231.4249313.png  \n",
            "  inflating: Dataset/1644441336.4059145.json  \n",
            "  inflating: Dataset/1644441336.4059145.png  \n",
            "  inflating: Dataset/1644441368.8287382.json  \n",
            "  inflating: Dataset/1644441368.8287382.png  \n",
            "  inflating: Dataset/1644441380.6067047.json  \n",
            "  inflating: Dataset/1644441380.6067047.png  \n",
            "  inflating: Dataset/1644441431.8396883.json  \n",
            "  inflating: Dataset/1644441431.8396883.png  \n",
            "  inflating: Dataset/1644441437.5057967.json  \n",
            "  inflating: Dataset/1644441437.5057967.png  \n",
            "  inflating: Dataset/1644441448.337717.json  \n",
            "  inflating: Dataset/1644441448.337717.png  \n",
            "  inflating: Dataset/1644441464.4244668.json  \n",
            "  inflating: Dataset/1644441464.4244668.png  \n",
            "  inflating: Dataset/1644441485.2553048.json  \n",
            "  inflating: Dataset/1644441485.2553048.png  \n",
            "  inflating: Dataset/1644441522.2447002.json  \n",
            "  inflating: Dataset/1644441522.2447002.png  \n",
            "  inflating: Dataset/1644441902.2140336.json  \n",
            "  inflating: Dataset/1644441902.2140336.png  \n",
            "  inflating: Dataset/1644441923.8823192.json  \n",
            "  inflating: Dataset/1644441923.8823192.png  \n",
            "  inflating: Dataset/1644442031.3938398.json  \n",
            "  inflating: Dataset/1644442031.3938398.png  \n",
            "  inflating: Dataset/1644442071.398712.json  \n",
            "  inflating: Dataset/1644442071.398712.png  \n",
            "  inflating: Dataset/1644442073.9465039.json  \n",
            "  inflating: Dataset/1644442073.9465039.png  \n",
            "  inflating: Dataset/1644442198.7625072.json  \n",
            "  inflating: Dataset/1644442198.7625072.png  \n",
            "  inflating: Dataset/1644442201.424109.json  \n",
            "  inflating: Dataset/1644442201.424109.png  \n",
            "  inflating: Dataset/1644442231.5471883.json  \n",
            "  inflating: Dataset/1644442231.5471883.png  \n",
            "  inflating: Dataset/1644443458.6042314.json  \n",
            "  inflating: Dataset/1644443458.6042314.png  \n",
            "  inflating: Dataset/1644444186.598305.json  \n",
            "  inflating: Dataset/1644444186.598305.png  \n",
            "  inflating: Dataset/1644444229.1643362.json  \n",
            "  inflating: Dataset/1644444229.1643362.png  \n",
            "  inflating: Dataset/1644444267.6504297.json  \n",
            "  inflating: Dataset/1644444267.6504297.png  \n",
            "  inflating: Dataset/1644444437.2468452.json  \n",
            "  inflating: Dataset/1644444437.2468452.png  \n",
            "  inflating: Dataset/1644444918.824118.json  \n",
            "  inflating: Dataset/1644444918.824118.png  \n",
            "  inflating: Dataset/1644445129.8578093.json  \n",
            "  inflating: Dataset/1644445129.8578093.png  \n",
            "  inflating: Dataset/1644445370.781552.json  \n",
            "  inflating: Dataset/1644445370.781552.png  \n",
            "  inflating: Dataset/1644445378.7603095.json  \n",
            "  inflating: Dataset/1644445378.7603095.png  \n",
            "  inflating: Dataset/1644445438.6949565.json  \n",
            "  inflating: Dataset/1644445438.6949565.png  \n",
            "  inflating: Dataset/1644445509.6784966.json  \n",
            "  inflating: Dataset/1644445509.6784966.png  \n",
            "  inflating: Dataset/1644445517.3830535.json  \n",
            "  inflating: Dataset/1644445517.3830535.png  \n",
            "  inflating: Dataset/1644445546.5142424.json  \n",
            "  inflating: Dataset/1644445546.5142424.png  \n",
            "  inflating: Dataset/1644445656.1544516.json  \n",
            "  inflating: Dataset/1644445656.1544516.png  \n",
            "  inflating: Dataset/1644445847.9778368.json  \n",
            "  inflating: Dataset/1644445847.9778368.png  \n",
            "  inflating: Dataset/1644446166.839785.json  \n",
            "  inflating: Dataset/1644446166.839785.png  \n",
            "  inflating: Dataset/1644446222.3855703.json  \n",
            "  inflating: Dataset/1644446222.3855703.png  \n",
            "  inflating: Dataset/1644446604.979261.json  \n",
            "  inflating: Dataset/1644446604.979261.png  \n",
            "  inflating: Dataset/1644446717.9663947.json  \n",
            "  inflating: Dataset/1644446717.9663947.png  \n",
            "  inflating: Dataset/1644446780.446302.json  \n",
            "  inflating: Dataset/1644446780.446302.png  \n",
            "  inflating: Dataset/1644446835.1520977.json  \n",
            "  inflating: Dataset/1644446835.1520977.png  \n",
            "  inflating: Dataset/1644447048.9076676.json  \n",
            "  inflating: Dataset/1644447048.9076676.png  \n",
            "  inflating: Dataset/1644447052.502617.json  \n",
            "  inflating: Dataset/1644447052.502617.png  \n",
            "  inflating: Dataset/1644447448.6055446.json  \n",
            "  inflating: Dataset/1644447448.6055446.png  \n",
            "  inflating: Dataset/1644448213.076151.json  \n",
            "  inflating: Dataset/1644448213.076151.png  \n",
            "  inflating: Dataset/1644448309.5870373.json  \n",
            "  inflating: Dataset/1644448309.5870373.png  \n",
            "  inflating: Dataset/1644448357.0968075.json  \n",
            "  inflating: Dataset/1644448357.0968075.png  \n",
            "  inflating: Dataset/1644448389.611119.json  \n",
            "  inflating: Dataset/1644448389.611119.png  \n",
            "  inflating: Dataset/1644448440.0211878.json  \n",
            "  inflating: Dataset/1644448440.0211878.png  \n",
            "  inflating: Dataset/1644448489.5840967.json  \n",
            "  inflating: Dataset/1644448489.5840967.png  \n",
            "  inflating: Dataset/1644448758.2149394.json  \n",
            "  inflating: Dataset/1644448758.2149394.png  \n",
            "  inflating: Dataset/1644448903.2855551.json  \n",
            "  inflating: Dataset/1644448903.2855551.png  \n",
            "  inflating: Dataset/1644449005.55074.json  \n",
            "  inflating: Dataset/1644449005.55074.png  \n",
            "  inflating: Dataset/1644449241.8868785.json  \n",
            "  inflating: Dataset/1644449241.8868785.png  \n",
            "  inflating: Dataset/1644449271.6856074.json  \n",
            "  inflating: Dataset/1644449271.6856074.png  \n",
            "  inflating: Dataset/1644449518.711144.json  \n",
            "  inflating: Dataset/1644449518.711144.png  \n",
            "  inflating: Dataset/1644449567.1379383.json  \n",
            "  inflating: Dataset/1644449567.1379383.png  \n",
            "  inflating: Dataset/1644449774.8806453.json  \n",
            "  inflating: Dataset/1644449774.8806453.png  \n",
            "  inflating: Dataset/1644451099.105409.json  \n",
            "  inflating: Dataset/1644451099.105409.png  \n",
            "  inflating: Dataset/1644451115.2836092.json  \n",
            "  inflating: Dataset/1644451115.2836092.png  \n",
            "  inflating: Dataset/1644451213.890201.json  \n",
            "  inflating: Dataset/1644451213.890201.png  \n",
            "  inflating: Dataset/1644451265.78203.json  \n",
            "  inflating: Dataset/1644451265.78203.png  \n",
            "  inflating: Dataset/1644451279.526318.json  \n",
            "  inflating: Dataset/1644451279.526318.png  \n",
            "  inflating: Dataset/1644451583.43769.json  \n",
            "  inflating: Dataset/1644451583.43769.png  \n",
            "  inflating: Dataset/1644451795.3529818.json  \n",
            "  inflating: Dataset/1644451795.3529818.png  \n",
            "  inflating: Dataset/1644457613.8134043.json  \n",
            "  inflating: Dataset/1644457613.8134043.png  \n",
            "  inflating: Dataset/1644462046.758302.json  \n",
            "  inflating: Dataset/1644462046.758302.png  \n",
            "  inflating: Dataset/1644462521.4657695.json  \n",
            "  inflating: Dataset/1644462521.4657695.png  \n",
            "  inflating: Dataset/1644465308.5303612.json  \n",
            "  inflating: Dataset/1644465308.5303612.png  \n",
            "  inflating: Dataset/1644469719.8941123.json  \n",
            "  inflating: Dataset/1644469719.8941123.png  \n",
            "  inflating: Dataset/1644479053.8743942.json  \n",
            "  inflating: Dataset/1644479053.8743942.png  \n",
            "  inflating: Dataset/1644481862.3811753.json  \n",
            "  inflating: Dataset/1644481862.3811753.png  \n",
            "  inflating: Dataset/1644483278.1318731.json  \n",
            "  inflating: Dataset/1644483278.1318731.png  \n",
            "  inflating: Dataset/1644483384.162388.json  \n",
            "  inflating: Dataset/1644483384.162388.png  \n",
            "  inflating: Dataset/1644484268.1963873.json  \n",
            "  inflating: Dataset/1644484268.1963873.png  \n",
            "  inflating: Dataset/1644490473.822668.json  \n",
            "  inflating: Dataset/1644490473.822668.png  \n",
            "  inflating: Dataset/1644494415.8383284.json  \n",
            "  inflating: Dataset/1644494415.8383284.png  \n",
            "  inflating: Dataset/1644495169.1201317.json  \n",
            "  inflating: Dataset/1644495169.1201317.png  \n",
            "  inflating: Dataset/1644875773.6795602.bmp  \n",
            "  inflating: Dataset/1644875773.6795602.json  \n",
            "  inflating: Dataset/1644876249.3602998.bmp  \n",
            "  inflating: Dataset/1644876249.3602998.json  \n",
            "  inflating: Dataset/1644878967.7489526.bmp  \n",
            "  inflating: Dataset/1644878967.7489526.json  \n",
            "  inflating: Dataset/1644879631.817003.bmp  \n",
            "  inflating: Dataset/1644879631.817003.json  \n",
            "  inflating: Dataset/1644880490.5008993.bmp  \n",
            "  inflating: Dataset/1644880490.5008993.json  \n",
            "  inflating: Dataset/1644888240.3566377.bmp  \n",
            "  inflating: Dataset/1644888240.3566377.json  \n",
            "  inflating: Dataset/1644888256.0602736.bmp  \n",
            "  inflating: Dataset/1644888256.0602736.json  \n",
            "  inflating: Dataset/1645634834.280746.json  \n",
            "  inflating: Dataset/1645634834.280746.png  \n",
            "  inflating: Dataset/1645635002.2771971.json  \n",
            "  inflating: Dataset/1645635002.2771971.png  \n",
            "  inflating: Dataset/1645635060.518038.json  \n",
            "  inflating: Dataset/1645635060.518038.png  \n",
            "  inflating: Dataset/1645635105.105883.json  \n",
            "  inflating: Dataset/1645635105.105883.png  \n",
            "  inflating: Dataset/1645635195.07319.json  \n",
            "  inflating: Dataset/1645635195.07319.png  \n",
            "  inflating: Dataset/1645635304.7972324.json  \n",
            "  inflating: Dataset/1645635304.7972324.png  \n",
            "  inflating: Dataset/1645635403.8522236.json  \n",
            "  inflating: Dataset/1645635403.8522236.png  \n",
            "  inflating: Dataset/1645635488.3734396.json  \n",
            "  inflating: Dataset/1645635488.3734396.png  \n",
            "  inflating: Dataset/1645635557.7693658.json  \n",
            "  inflating: Dataset/1645635557.7693658.png  \n",
            "  inflating: Dataset/1645635687.192301.json  \n",
            "  inflating: Dataset/1645635687.192301.png  \n",
            "  inflating: Dataset/1645635923.026066.json  \n",
            "  inflating: Dataset/1645635923.026066.png  \n",
            "  inflating: Dataset/1645636016.6718738.json  \n",
            "  inflating: Dataset/1645636016.6718738.png  \n",
            "Archive:  /content/Patterns.zip\n",
            "  inflating: Patterns/AYLIN.tif      \n",
            "  inflating: Patterns/Orlando_60x60-1.tif  \n",
            "  inflating: Patterns/Orlando_60x60-10.tif  \n",
            "  inflating: Patterns/Orlando_60x60-2.tif  \n",
            "  inflating: Patterns/Orlando_60x60-3.tif  \n",
            "  inflating: Patterns/Orlando_60x60-5.tif  \n",
            "  inflating: Patterns/Orlando_60x60-6.tif  \n",
            "  inflating: Patterns/Orlando_60x60-7.tif  \n",
            "  inflating: Patterns/Orlando_60x60-8.tif  \n",
            "  inflating: Patterns/Orlando_60x60-9.tif  \n",
            "  inflating: Patterns/REJINA.tif     \n",
            "  inflating: Patterns/sama.tif       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read image and pattern"
      ],
      "metadata": {
        "id": "G51qxhLeI6Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import json"
      ],
      "metadata": {
        "id": "Isx9_evLIvT0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name = \"/content/Dataset/1644362482.7094185.jpg\"\n",
        "json_name = img_name[:-3] + \"json\""
      ],
      "metadata": {
        "id": "pe-eLWBlI08H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv.imread(img_name)\n",
        "\n",
        "f = open(json_name, encoding=\"utf8\")\n",
        "data = json.load(f)\n",
        "f.close()\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(img[..., ::-1])\n",
        "\n",
        "for shape in data[\"shapes\"]: # array of json include taraks\n",
        "  # shape :\n",
        "  # {\n",
        "  #           \"label\": \"tarak\",\n",
        "  #           \"points\": [\n",
        "  #               [\n",
        "  #                   1239.6180419921875,\n",
        "  #                   768.5914306640625\n",
        "  #               ],\n",
        "  #               [\n",
        "  #                   1298.614990234375,\n",
        "  #                   758.45849609375\n",
        "  #               ],\n",
        "  #               [\n",
        "  #                   1300.58203125,\n",
        "  #                   769.4680786132812\n",
        "  #               ],\n",
        "  #               [\n",
        "  #                   1233.1680908203125,\n",
        "  #                   777.0652465820312\n",
        "  #               ]\n",
        "  #           ],\n",
        "  #           \"group_id\": null,\n",
        "  #           \"shape_type\": \"polygon\",\n",
        "  #           \"flags\": {}\n",
        "  #       }\n",
        "  points = np.array(shape['points'])\n",
        "  ind = list(np.arange(points.shape[0])) + [0]\n",
        "  plt.plot(points[ind, 0], points[ind, 1], 'r.-')\n",
        "  # plt.plot(points[-1:-3:-1, 0], points[-1:-3:-1, 1], 'r')"
      ],
      "metadata": {
        "id": "As1gQvjQI2RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = cv.imread('/content/Patterns/' + data['pattern']) # e.g. \"AYLIN.tif\"\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(pattern[..., ::-1])"
      ],
      "metadata": {
        "id": "y3B2kBSRI5c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Sequence class to load & vectorize batches of data"
      ],
      "metadata": {
        "id": "8CznuJGIK01s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_target(input_img_path):\n",
        "  img = cv.imread(input_img_path)\n",
        "  input_json_path = input_img_path[:-3] + 'json'\n",
        "  f = open(input_json_path, encoding=\"utf8\")\n",
        "  data = json.load(f)\n",
        "  f.close()\n",
        "\n",
        "  shapes = data[\"shapes\"]\n",
        "  for shape in shapes:\n",
        "    if shape[\"label\"] == \"tarak\":\n",
        "      points = np.array(shape[\"points\"]) # taraks\n",
        "      cv.fillPoly(img, pts=np.int32([points]), color=(0, 0, 255)) # Draw a filled polygon\n",
        "\n",
        "  indices = np.where(img != (0, 0, 255))\n",
        "  img[indices] = 0\n",
        "\n",
        "  indices = np.where(img == (0, 0, 255))\n",
        "  img[indices] = 128\n",
        "\n",
        "  gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "  i1 = np.where(gray == gray[0, 0])\n",
        "  i2 = np.where(gray != gray[0, 0])\n",
        "  gray[i1] = 0 # not taraks\n",
        "  gray[i2] = 128 # taraks\n",
        "\n",
        "  return gray.copy()"
      ],
      "metadata": {
        "id": "-4DpmkY8QEwY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/Dataset/1644360063.82364.jpg'\n",
        "target = create_target(img_path)\n",
        "plt.imshow(target, cmap=\"gray\")"
      ],
      "metadata": {
        "id": "joctPrjJIhj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import os\n",
        "\n",
        "class Tiles(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, is_pattern):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.is_pattern = is_pattern\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"uint8\") # (100, 1600, 1600, 3)\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\") # (100, 1600, 1600, 1)\n",
        "        pad = 70\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            # input img\n",
        "            img = load_img(path)\n",
        "            img = np.array(img)\n",
        "            img = img[pad:-pad, pad:-pad] # crop the paddings\n",
        "            img = cv.resize(img, self.img_size, interpolation=cv.INTER_AREA) # resize\n",
        "            x[j] = img\n",
        "\n",
        "            # target img\n",
        "            target_img = create_target(path) # our defined function\n",
        "            target_img = target_img[pad:-pad, pad:-pad].copy()\n",
        "            target_img = cv.resize(target_img, self.img_size, interpolation=cv.INTER_AREA)\n",
        "            indices = np.where(target_img != 0)\n",
        "            target_img[indices] = 1 # tarak (127 -> 1)\n",
        "            y[j] = np.expand_dims(target_img, 2) # Expand the shape of an array. 1 -> [1] [ (1600,1600, ) -> (1600,1600,1) ]\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "dxkIzo3jCZMZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare U-Net Xception-style model"
      ],
      "metadata": {
        "id": "Bvq0ec62LAHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/code/meaninglesslives/unet-xception-keras-for-pneumothorax-segmentation/notebook\n",
        "# https://www.researchgate.net/figure/U-Net-architecture-X-ception-style_fig4_364690133\n",
        "# https://github.com/cnzakimuena/U-Net\n",
        "# https://pyimagesearch.com/2022/02/21/u-net-image-segmentation-in-keras/\n",
        "num_classes = 2 # tarak, not tarak\n",
        "batch_size = 2\n",
        "img_size = (1600, 1600)"
      ],
      "metadata": {
        "id": "ADVOivTHBB2U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import glob\n",
        "from skimage import exposure\n",
        "from skimage.exposure import match_histograms\n",
        "from skimage import data\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "import keras\n",
        "import random\n",
        "import keras.backend as K\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    \"\"\"Find the Euclidean distance between two vectors.\n",
        "\n",
        "    Arguments:\n",
        "        vects: List containing two tensors of same length.\n",
        "\n",
        "    Returns:\n",
        "        Tensor containing euclidean distance\n",
        "        (as floating point value) between vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = vects\n",
        "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
        "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
        "\n",
        "\n",
        "input = layers.Input((1600, 1600, 3))\n",
        "x = tf.keras.layers.BatchNormalization()(input)\n",
        "x = layers.Conv2D(4, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Conv2D(16, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Conv2D(32, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Conv2D(64, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Conv2D(128, (5, 5), activation=\"relu\")(x)\n",
        "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = layers.Dense(10000, activation=\"relu\")(x)\n",
        "embedding_network = keras.Model(input, x)\n",
        "\n",
        "\n",
        "input_1 = layers.Input((1600, 1600, 3))\n",
        "input_2 = layers.Input((1600, 1600, 3))\n",
        "\n",
        "# As mentioned above, Siamese Network share weights between\n",
        "# tower networks (sister networks). To allow this, we will use\n",
        "# same embedding network for both tower networks.\n",
        "\n",
        "tower_1 = embedding_network(input_1)\n",
        "tower_2 = embedding_network(input_2)\n",
        "\n",
        "merge_layer = layers.Lambda(euclidean_distance)([tower_1, tower_2])\n",
        "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
        "output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
        "siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)"
      ],
      "metadata": {
        "id": "uCTWsdos_STF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(margin=1):\n",
        "    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
        "\n",
        "    Arguments:\n",
        "        margin: Integer, defines the baseline for distance for which pairs\n",
        "                should be classified as dissimilar. - (default is 1).\n",
        "\n",
        "    Returns:\n",
        "        'constrastive_loss' function with data ('margin') attached.\n",
        "    \"\"\"\n",
        "\n",
        "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
        "    #                         true_value * square( max(margin-prediction, 0) ))\n",
        "    def contrastive_loss(y_true, y_pred):\n",
        "        \"\"\"Calculates the constrastive loss.\n",
        "\n",
        "        Arguments:\n",
        "            y_true: List of labels, each label is of type float32.\n",
        "            y_pred: List of predictions of same length as of y_true,\n",
        "                    each label is of type float32.\n",
        "\n",
        "        Returns:\n",
        "            A tensor containing constrastive loss as floating point value.\n",
        "        \"\"\"\n",
        "\n",
        "        square_pred = tf.math.square(y_pred)\n",
        "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
        "        return tf.math.reduce_mean(\n",
        "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
        "        )\n",
        "\n",
        "    return contrastive_loss\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "  def recall(y_true, y_pred):\n",
        "      \"\"\"Recall metric.\n",
        "\n",
        "      Only computes a batch-wise average of recall.\n",
        "\n",
        "      Computes the recall, a metric for multi-label classification of\n",
        "      how many relevant items are selected.\n",
        "      \"\"\"\n",
        "      true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "      possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "      recall = true_positives / (possible_positives + K.epsilon())\n",
        "      return recall\n",
        "\n",
        "  def precision(y_true, y_pred):\n",
        "      \"\"\"Precision metric.\n",
        "\n",
        "      Only computes a batch-wise average of precision.\n",
        "\n",
        "      Computes the precision, a metric for multi-label classification of\n",
        "      how many selected items are relevant.\n",
        "      \"\"\"\n",
        "      true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "      predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "      precision = true_positives / (predicted_positives + K.epsilon())\n",
        "      return precision\n",
        "  precision = precision(y_true, y_pred)\n",
        "  recall = recall(y_true, y_pred)\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "margin = 1\n",
        "siamese.compile(loss=loss(margin=margin), optimizer=\"adam\", metrics=[\"accuracy\", f1])\n",
        "siamese.summary()"
      ],
      "metadata": {
        "id": "tUowM7qlFu_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(siamese, show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "M9sLACiiF18E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Free up RAM in case the model definition cells were run multiple times\n",
        "# keras.backend.clear_session()\n",
        "\n",
        "# # Build model\n",
        "# model = get_model(img_size, num_classes)\n",
        "# model.summary()\n",
        "# keras.utils.plot_model(model)"
      ],
      "metadata": {
        "id": "NFMqcpz4GjOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Prepared And Augmented Data Again"
      ],
      "metadata": {
        "id": "_O2Xer3ZN7ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"/content/Dataset\"\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if not fname.endswith(\".json\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "\n",
        "for input_path in input_img_paths[-15:]:\n",
        "    print(input_path, \"|\", input_path[:-3] + 'json')"
      ],
      "metadata": {
        "id": "eZT8Xq_aN6zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set aside a validation split"
      ],
      "metadata": {
        "id": "o3w2hBhbNTUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 50\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "# random.Random(0).shuffle(train_input_img_paths)\n",
        "# random.Random(0).shuffle(val_input_img_paths)\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_tile_gen = Tiles(batch_size, img_size, train_input_img_paths, False) #  \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "train_pattern_gen = Tiles(batch_size, img_size, train_input_img_paths, True) #  \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "val_gen = Tiles(batch_size, img_size, val_input_img_paths, False) #  \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\""
      ],
      "metadata": {
        "id": "CJROgOx3MKQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See Tile & Pattern & Mask"
      ],
      "metadata": {
        "id": "mF9PmmJdO22n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30, 30))\n",
        "plt.subplot(2, 1, 1)\n",
        "imgs, targets = train_gen[7]\n",
        "plt.imshow(imgs[0]) # imgs[1]\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.imshow(targets[0], cmap=\"gray\") # targets[1]"
      ],
      "metadata": {
        "id": "9hNRnt9-O3J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)"
      ],
      "metadata": {
        "id": "7RyUv2OaF3gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=0.01):\n",
        "  # ref: https://lars76.github.io/2018/09/27/loss-functions-for-segmentation.html#6\n",
        "  # (batch_size, img1, img2, num_class)\n",
        "  y_true = tf.cast(y_true, tf.float32)\n",
        "  y_pred = tf.math.sigmoid(y_pred)\n",
        "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
        "  denominator = tf.reduce_sum(y_true + y_pred) # DC ≥ IoU\n",
        "\n",
        "  return (numerator + smooth) / (denominator + smooth)\n",
        "\n",
        "def dice_bce_loss(y_true, y_pred):\n",
        "    \"\"\"combine DICE and BCE\"\"\"\n",
        "    # ref: https://www.kaggle.com/code/kmader/u-net-with-dice-and-augmentation/notebook\n",
        "    dice_loss =  1 - dice_coef(y_true, y_pred)\n",
        "    return 0.01 * binary_crossentropy(y_true, y_pred) + dice_loss"
      ],
      "metadata": {
        "id": "W2QdkXkNrSlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "2jZnFvpdRKfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# save best weights\n",
        "# ref: https://towardsdatascience.com/keras-callbacks-and-how-to-save-your-model-from-overtraining-244fc1de8608\n",
        "filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=filepath,\n",
        "    monitor='val_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_bce_loss, metrics=[dice_coef])\n",
        "\n",
        "epochs = 10\n",
        "history = model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "PcoF9KELnHTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = siamese.fit(\n",
        "    [train1, train2],\n",
        "    val_gen,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "i7s3o8C1GQM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save latest weights\n",
        "model.save_weights('unet.h5')"
      ],
      "metadata": {
        "id": "RnSH2MpsGVxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(history.history['dice_coef'])\n",
        "plt.plot(history.history['val_dice_coef'])\n",
        "plt.title('model dice_coef')\n",
        "plt.ylabel('dice_coef')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "803ixtpxKGoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training history\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('dice_bce_loss')\n",
        "plt.savefig('model_training_history')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9jSRcfmzS6bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize predictions\n"
      ],
      "metadata": {
        "id": "Vgf2XYI4Vs1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load best weights\n",
        "keras.backend.clear_session()\n",
        "filepath = \"my_best_model.epoch05-loss0.72.hdf5\"\n",
        "model = keras.models.load_model(filepath, custom_objects={\"dice_bce_loss\": dice_bce_loss, \"dice_coef\": dice_coef})"
      ],
      "metadata": {
        "id": "Oz9Aur1UTJf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageOps\n",
        "\n",
        "# Generate predictions for all images in the validation set\n",
        "val_gen = Tiles(batch_size, img_size, val_input_img_paths) # 50 images (25 batches)\n",
        "val_preds = model.predict(val_gen)"
      ],
      "metadata": {
        "id": "Mr7DiZFiVF-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18bbb67-efff-47dc-91c8-d8ced636e940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 33s 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "def get_visualization(i, img):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    probability = 1e-20\n",
        "    predict = img.copy()\n",
        "    mask = val_preds[i]\n",
        "    mask = tf.math.sigmoid(mask)\n",
        "    mask = np.array(mask)\n",
        "\n",
        "    indices = np.where(mask >= probability)\n",
        "    mask[indices] = 128\n",
        "\n",
        "    indices = indices[0], indices[1]\n",
        "    predict[indices] = (255, 0, 0)\n",
        "\n",
        "    return mask, predict\n",
        "\n",
        "# Display results for validation image #i\n",
        "i = 0\n",
        "batch = i // batch_size\n",
        "j = i % batch_size\n",
        "imgs, targets = val_gen[batch]\n",
        "\n",
        "# Display input image\n",
        "plt.figure(figsize=(30, 30))\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.imshow(imgs[j])\n",
        "\n",
        "# Display ground-truth target mask\n",
        "plt.subplot(4, 1, 2)\n",
        "plt.imshow(targets[j], cmap=\"gray\")\n",
        "\n",
        "# Display mask predicted by our model\n",
        "plt.subplot(4, 1, 3)\n",
        "mask, prediction = get_visualization(i, imgs[j])\n",
        "plt.imshow(mask, cmap=\"gray\")\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "plt.imshow(prediction)"
      ],
      "metadata": {
        "id": "flxzw5jyjlw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define predict function\n",
        "\n",
        "# define predict function\n",
        "def my_get_visualization(img, mask):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    probability = 1e-20\n",
        "    predict = img.copy()\n",
        "    mask = tf.math.sigmoid(mask)\n",
        "    mask = np.array(mask)\n",
        "\n",
        "    indices = np.where(mask >= probability)\n",
        "    mask[indices] = 128\n",
        "\n",
        "    indices = indices[0], indices[1]\n",
        "    predict[indices] = (255, 0, 0)\n",
        "\n",
        "    return mask, predict\n",
        "\n",
        "\n",
        "def predict_func(image, pattern):\n",
        "    '''\n",
        "    Receive the corresponding image and design and return the coordinates of the cracks inside it\n",
        "    '''\n",
        "\n",
        "    # load best weights\n",
        "    keras.backend.clear_session()\n",
        "    filepath = \"my_best_model.epoch05-loss0.72.hdf5\"\n",
        "    model = keras.models.load_model(filepath, custom_objects={\"dice_bce_loss\": dice_bce_loss, \"dice_coef\": dice_coef})\n",
        "\n",
        "    # load image\n",
        "    img = Image.open(image)\n",
        "    img = img.convert('RGB')\n",
        "    img = np.array(img)\n",
        "    img = img / 255.0\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    # # load pattern\n",
        "    # pattern = Image.open(pattern)\n",
        "    # pattern = pattern.convert('RGB')\n",
        "    # pattern = np.array(pattern)\n",
        "    # pattern = pattern / 255.0\n",
        "    # pattern = pattern.astype(np.float32)\n",
        "\n",
        "    # predict\n",
        "    # img = np.expand_dims(img, axis=0)\n",
        "    # pattern = np.expand_dims(pattern, axis=0)\n",
        "    # img = np.concatenate((img, pattern), axis=0)\n",
        "    # img = np.expand_dims(img, axis=0)\n",
        "    mask = model.predict(img)\n",
        "\n",
        "    # method 1\n",
        "    # mask, prediction = my_get_visualization(img[0, :, :, :], mask[0, :, :, :])\n",
        "    # mask = np.where(mask >= 0.5, 1, 0)\n",
        "    # mask = mask.astype(np.uint8)\n",
        "\n",
        "    # method 2\n",
        "    mask = tf.math.sigmoid(mask)\n",
        "    mask = np.array(mask)\n",
        "    mask = mask[0, :, :, 0]\n",
        "    mask = np.where(mask >= 0.5, 1, 0)\n",
        "    mask = mask.astype(np.uint8)\n",
        "\n",
        "    # get coordinates\n",
        "    contours, hierarchy = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    coordinates = []\n",
        "    for i in range(len(contours)):\n",
        "        x, y, w, h = cv.boundingRect(contours[i])\n",
        "        coordinates.append([x, y, x+w, y+h])\n",
        "\n",
        "    return coordinates\n"
      ],
      "metadata": {
        "id": "Em6pFerZLQdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}